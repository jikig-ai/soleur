# SEO & AEO for Docs Website

**Date:** 2026-02-19
**Status:** Active
**Issue:** #131

## What We're Building

A reusable SEO/AEO (Search Engine Optimization + AI Engine Optimization) skill and agent for Eleventy documentation sites, with Soleur's docs at soleur.ai as the first customer.

The **skill** (`seo-aeo`) provides sub-commands for the developer workflow:
- `audit` -- scans Eleventy source + built `_site/`, reports issues with severity
- `fix` -- generates fixes for all found issues (meta tags, JSON-LD, sitemap, llms.txt, content rewrites)
- `validate` -- lightweight check mode for CI integration (exit code 0/1)

The **agent** (`seo-aeo-analyst`) handles the analytical heavy lifting:
- Crawl the site structure and score pages
- Identify content gaps and optimization opportunities
- Generate structured data (JSON-LD, schema.org)
- Create/optimize llms.txt for AI crawlers
- Rewrite content sections for AI extractability (FAQ, definitions, comparisons)

A **CI validation step** in `deploy-docs.yml` runs the validate sub-command on every deploy to prevent SEO regressions.

## Why This Approach

**Skill-first:** Build the general tool, then apply it. This aligns with Soleur's vision of making capabilities available to all users, not just fixing our own site.

**Skill + agent split:** The skill handles the workflow (sub-commands, CI integration, user interaction). The agent handles the intelligence (analysis, content generation, scoring). This lets other skills call the agent directly when they need SEO insights.

**Eleventy-focused:** The skill understands Eleventy's data cascade, Nunjucks templates, and config system. It can generate `_data` files, template partials, and `eleventy.config.js` changes -- not just recommendations.

**Full pipeline:** Audit finds issues, fix resolves them, validate prevents regressions. The full loop closes the feedback cycle.

## Key Decisions

1. **Skill-first, not soleur-first.** Build the reusable tool, then apply it to soleur.ai as first customer.
2. **Skill + dedicated agent architecture.** Skill for workflow/sub-commands, agent for analysis/generation.
3. **Eleventy-focused.** The skill understands Eleventy internals, not just raw HTML output.
4. **Full pipeline with CI.** Audit + fix + validate. CI step in deploy-docs.yml prevents regressions.
5. **Generate content directly.** The skill generates content (FAQ sections, rewrites, structured data) and applies them. User reviews the diff.
6. **Both SEO and AEO.** Structured data (JSON-LD, schema.org) for machine readability AND content optimization (llms.txt, FAQ, definitions) for AI citation quality.

## Current State of soleur.ai

### What exists
- Meta descriptions (per-page via frontmatter)
- Open Graph tags (title, description, url, type, image)
- robots.txt (allows all, references sitemap)
- sitemap.xml (5 URLs, static Nunjucks template)
- CNAME for custom domain
- Accessible markup (lang, skip link, aria-current)

### What's missing
- No Twitter/X card meta tags
- No canonical URLs (`<link rel="canonical">`)
- No JSON-LD structured data (no schema.org markup)
- No `<lastmod>` or `<changefreq>` in sitemap
- No llms.txt for AI crawlers
- Changelog page is client-side rendered (invisible to crawlers without JS)
- Single OG image for all pages (no per-page images)
- No RSS/Atom feed
- No breadcrumb navigation for inner pages
- Deploy CI does not verify sitemap.xml or robots.txt

### Known gotchas (from learnings)
- Nunjucks variables do NOT resolve inside YAML frontmatter -- dynamic meta tag content must be in the template body or via computed data
- URL concatenation: `{{ site.url }}{{ page.url }}` -- no separator needed, page.url starts with `/`
- Eleventy v3 is ESM-based -- all config/data files must use `export default`

## Scope for Skill (v1)

### SEO audit checks
- Meta tags: title, description, canonical, OG, Twitter cards
- Structured data: JSON-LD for WebSite, SoftwareApplication, Organization, BreadcrumbList
- Sitemap: presence, lastmod dates, all pages included
- robots.txt: presence, references sitemap, correct domain
- Heading hierarchy: h1-h6 order, no skips
- Image alt text
- Internal link structure

### AEO audit checks
- llms.txt: presence, completeness, accuracy
- Content extractability: clear definitions, FAQ sections, comparison framing
- Schema.org completeness for AI model understanding
- Page structure: machine-readable sections with clear headings

### Fix capabilities
- Generate/update JSON-LD templates
- Generate/update llms.txt
- Add Twitter card meta tags
- Add canonical URLs
- Enhance sitemap with lastmod dates
- Generate FAQ sections from existing content
- Rewrite descriptions for AI extractability

### CI validation (validate sub-command)
- sitemap.xml exists and has lastmod dates
- Every page has meta description + canonical + OG tags
- JSON-LD validates against schema.org
- llms.txt exists and is non-empty
- robots.txt references sitemap with correct domain

## Open Questions

1. **llms.txt format:** The llms.txt standard is still emerging. Should we follow the proposed spec at llms-txt.org or create a more comprehensive format?
2. **Per-page OG images:** Should the skill auto-generate per-page OG images (e.g., using a template with the page title), or is the shared image sufficient for v1?
3. **RSS feed:** Should the skill generate an RSS/Atom feed for the changelog, or is that out of scope for v1?
4. **Breadcrumbs:** The site has a flat structure (5 pages). Are breadcrumbs worth it, or only for deeper sites?
